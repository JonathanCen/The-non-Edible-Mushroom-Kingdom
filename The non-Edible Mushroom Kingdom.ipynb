{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "#### Side note: `pip install graphviz` and reset kernel if an error occurs when importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Source\n",
    "from sklearn import tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up: Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the dataset\n",
    "dataset = pd.read_csv(\"mushrooms_imputed.csv\")\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get a description of the dataset\n",
    "display(dataset.describe(include='all'))\n",
    "\n",
    "# Create a helper function to visualize all the columns using seaborn library\n",
    "def visualize_column(col_n):\n",
    "    ## 2 Bar graphs\n",
    "    fig=plt.figure(figsize=(22, 10))\n",
    "    \n",
    "    ## Left graph\n",
    "    plt.subplot(221)\n",
    "    sns.countplot(x=col_n, data=dataset, order=dataset[col_n].value_counts().index, palette='rocket')\n",
    "    plt.title('Frequency of mushroom by {}'.format(col_n.replace('-', ' ')), fontsize=22, pad=10)\n",
    "    plt.xlabel(col_n.replace('-', ' ').capitalize(), fontsize=16)\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    ## Right graph\n",
    "    plt.subplot(222)\n",
    "    sns.countplot(x=col_n, data=dataset, hue='class', order=dataset[col_n].value_counts().index, palette='rocket')\n",
    "    plt.title('Frequency of {} by class'.format(col_n.replace('-', ' ')), fontsize=22, pad=10)\n",
    "    plt.legend(bbox_to_anchor=(1.15, 1), loc='upper right', fontsize=12)\n",
    "    plt.xlabel(col_n.replace('-', ' ').capitalize(), fontsize=16)\n",
    "    plt.ylabel('')\n",
    "    plt.show() \n",
    "\n",
    "# Dataset.columns[0] is the class so exclude it when visualizing data\n",
    "for col_names in list(dataset.columns)[1:]:\n",
    "    print(col_names)\n",
    "    visualize_column(col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation: Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Examine the dataset/ Check for any missing data/values\n",
    "shape = dataset.shape\n",
    "print(f'The dataset has {shape[0]} rows and {shape[1]} columns\\n')\n",
    "print(\"Check if there are any missing values:\")\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "## Print the Unique Types within each feature\n",
    "unique_df = pd.DataFrame()\n",
    "print('\\n{0:25} {1:25} {2}'.format('Column', '# Unique Types', 'Unique Types'))\n",
    "for col in dataset:\n",
    "    print('{0:32} {1} {2} {3}'.format(col, dataset[col].nunique(), ' '*15 , dataset[col].unique()))\n",
    "\n",
    "num_missing_entries = dataset.loc[dataset['stalk-root'] == '?', 'stalk-root'].count()\n",
    "total_entries = dataset['stalk-root'].count()\n",
    "print(f\"\\nNumber of data entries w/ stalk-root = '?': {num_missing_entries}\")\n",
    "print(f\"Total number of data entries: {total_entries} \")\n",
    "print(f\"Percentage of data entries w/ stalk-root = '?': {(num_missing_entries/total_entries)*100}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation: Data Cleaning (Fix missing value)\n",
    "> This was our inital way of handling the missing feature, but we modified the dataset externally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create a deep copy of the dataset\n",
    "clean_dataset_by_most_freq_feature = dataset.copy()\n",
    "\n",
    "# Replacing all feature values '?' w/ the most frequent feature == replacing all feature values '?' w/ the most frequent feature of the class\n",
    "# From the data visualization above 'b' is the most frequent feature\n",
    "clean_dataset_by_most_freq_feature = clean_dataset_by_most_freq_feature.replace('?', 'b')\n",
    "dataset = clean_dataset_by_most_freq_feature\n",
    "print(dataset['stalk-root'].unique())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Removing Veil Type\n",
    "#### Useless feature only one attribute type which both classes share (zero-variance predictors). Goal is to examine whether removing such feature will increase performance. Our prediction is it won't do much to the performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performace on this dataset as well...\n",
    "r_col_dataset = dataset.drop('veil-type', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data: Label encode the dataset\n",
    "#### Side note: Label encoding is converting each feature value into a numeric form and this is needed because sklearn libraries work only on numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create a deep copy of the dataset\n",
    "c_dataset = dataset.copy()\n",
    "\n",
    "# Split the dataset\n",
    "# c_X : a dataframe; c_y : a vector\n",
    "c_X = c_dataset.drop('class', axis = 1)\n",
    "c_y = c_dataset['class']\n",
    "\n",
    "# Create an instance of a OneHotEncoder (for categorical features) and LabelBinarizer (for labels)\n",
    "hot_encoder = OneHotEncoder()\n",
    "binarizer_encoder = LabelBinarizer()\n",
    "\n",
    "# Encode the features & values and labels\n",
    "enc_X = hot_encoder.fit_transform(c_X)\n",
    "enc_y = binarizer_encoder.fit_transform(c_y)\n",
    "\n",
    "# print(enc_X)\n",
    "\n",
    "# Checking the inverse encoding\n",
    "# inv_X = hot_encoder.inverse_transform(enc_X)\n",
    "# inv_y = binarizer_encoder.inverse_transform(enc_y)\n",
    "# inv_dataset = pd.DataFrame(inv_X, columns = list(c_X.columns))\n",
    "# inv_dataset.insert(loc = 0, column = 'class', value = inv_y)\n",
    "# display(inv_dataset.head())\n",
    "# display(c_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to training and validation set\n",
    "#### Side note: The row # and columns # is a bit off from the actual dataset because of the encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splits the data to a 2/3 for training and ~1/3 for testing\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(enc_X, enc_y, test_size = 0.3, random_state=0)\n",
    "\n",
    "# print(y_train)\n",
    "print(f'Dataset contains {enc_X.shape[0]} rows and {enc_X.shape[1]} columns')\n",
    "\n",
    "print(f'Attributes & values training data contains {X_train.shape[0]} rows and {X_train.shape[1]} columns')\n",
    "print(f'Class label training data contains {y_train.shape[0]} rows')\n",
    "\n",
    "print(f'Attributes & values test data contains {X_validation.shape[0]} rows and {X_validation.shape[1]} columns')\n",
    "print(f'Class label test data contains {y_validation.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subroutine function to label encode & split dataset\n",
    "#### Used to test any other datasets after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically does the two blocks above but in one function\n",
    "## dataset - is the dataset to split\n",
    "## test_size - is a float between [0, 1] representing the proportion of the dataset to include in the test split\n",
    "## returns:\n",
    "### X_train, X_validation, y_train, y_validation, OneHotEncoder, LabelBinarizer\n",
    "### OneHotEncoder, LabelBinarizer - allows for the data to be decoded if wanted\n",
    "def generate_random_split(dataset, test_size_val):\n",
    "    # Create a deep copy of the dataset\n",
    "    c_dataset = dataset.copy()\n",
    "\n",
    "    # Split the dataset\n",
    "    c_X = c_dataset.drop('class', axis = 1)\n",
    "    c_y = c_dataset['class']\n",
    "\n",
    "    # Create an instance of a OneHotEncoder (for categorical features) and LabelBinarizer (for labels)\n",
    "    hot_encoder = OneHotEncoder()\n",
    "    binarizer_encoder = LabelBinarizer()\n",
    "\n",
    "    # Encode the features & values and labels\n",
    "    enc_X = hot_encoder.fit_transform(c_X)\n",
    "    enc_y = binarizer_encoder.fit_transform(c_y)\n",
    "    \n",
    "    # Splits the data\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(enc_X, enc_y, test_size = test_size_val, random_state=0)\n",
    "    \n",
    "    return X_train, X_validation, y_train, y_validation, hot_encoder, binarizer_encoder, enc_X, enc_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode Dataset that Removed the 'veil-type' Feature \n",
    "#### Explore whether removing the column will provide better performance than not removing the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_X_train, r_X_validation, r_y_train, r_y_validation, r_hot_encoder, r_binarizer_encoder, r_enc_X, r_enc_y = generate_random_split(\n",
    "    r_col_dataset, 0.3)\n",
    "\n",
    "print(f'Attributes & values training data contains {r_X_train.shape[0]} rows and {r_X_train.shape[1]} columns')\n",
    "print(f'Class label training data contains {r_y_train.shape[0]} rows')\n",
    "\n",
    "print(f'Attributes & values test data contains {r_X_validation.shape[0]} rows and {r_X_validation.shape[1]} columns')\n",
    "print(f'Class label test data contains {r_y_validation.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode Dataset that replaced '?' w/ Most Frequent 'stalk-root' Feature \n",
    "#### Explore whether removing the column will provide better performance than not removing the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Again we found a different way of cleaning the data, this is not needed\n",
    "\"\"\"\n",
    "c_X_train, c_X_validation, c_y_train, c_y_validation, c_hot_encoder, c_binarizer_encoder, c_enc_X, c_enc_y = generate_random_split(\n",
    "    clean_dataset_by_most_freq_feature, 0.3)\n",
    "\n",
    "print(f'Attributes & values training data contains {r_X_train.shape[0]} rows and {r_X_train.shape[1]} columns')\n",
    "print(f'Class label training data contains {r_y_train.shape[0]} rows')\n",
    "\n",
    "print(f'Attributes & values test data contains {r_X_validation.shape[0]} rows and {r_X_validation.shape[1]} columns')\n",
    "print(f'Class label test data contains {r_y_validation.shape[0]} rows')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a float of how well the decision_tree performed on the feature_data compared to the expected_output\n",
    "# feature_data - is a matrix (2-d vector) of features values\n",
    "# expected_output - is a vector of the corresponding classes\n",
    "def measure_accuracy(model, feature_data, expected_output):\n",
    "    return accuracy_score(model.predict(feature_data), expected_output)\n",
    "\n",
    "## Helper function for testing n printing \n",
    "def evaluate_model_n_print_info(model, X_train, y_train, X_validation, y_validation):\n",
    "    start = time.time()\n",
    "    model_train_split_accuracy = measure_accuracy(model, X_train, y_train)\n",
    "    model_testing_training_time = time.time() - start\n",
    "    start = time.time()\n",
    "    model_validation_split_accuracy = measure_accuracy(model, X_validation, y_validation)\n",
    "    model_testing_validation_time = time.time() - start\n",
    "    print(f'Train Accuracy = {model_train_split_accuracy}')\n",
    "    print(f'Validation Accuracy = {model_validation_split_accuracy}')\n",
    "    print(f'Time elapse for testing on training data: {model_testing_training_time}')\n",
    "    print(f'Time elapsed for testing on validation data: {model_testing_validation_time}\\n')\n",
    "    return model_train_split_accuracy, model_validation_split_accuracy, model_testing_training_time, model_testing_validation_time\n",
    "\n",
    "# Returns a dataframe of the results so displaying it would be easier to see\n",
    "def df_k_fold_cross_validation(k_fold_results_arr, k_fold_time_arr):\n",
    "    d = {\"Accuracy for kth-fold\": k_fold_results_arr, \"Time for kth-fold\":k_fold_time_arr}\n",
    "    return pd.DataFrame(data = d)\n",
    "\n",
    "# K-fold cross validation helper function\n",
    "def k_fold_cross_validation(model, X_train, y_train):\n",
    "    # Recommended to do 10 folds; returns a dictionary of values\n",
    "    result = cross_validate(model, X_train, y_train, cv=10)\n",
    "    display(df_k_fold_cross_validation(result['test_score'], result['fit_time']))\n",
    "    model_mean_accuracy = round(sum(result['test_score']) / len(result['test_score']), 4)\n",
    "    model_mean_fit_time = round(sum(result['fit_time']) / len(result['fit_time']), 4)\n",
    "    print(f'Average accuracy for all K-fold cross-validations: {model_mean_accuracy}')\n",
    "    print(f'Time elapse for K-fold cross-validations: {model_mean_fit_time}.\\n')\n",
    "    return result\n",
    "\n",
    "# Grid Search helper function (used to capture the best hyperparameters for the model)\n",
    "def grid_serach_helper(model, parameters, X_train, y_train):\n",
    "    model_grid = GridSearchCV(model, parameters, cv = 10)\n",
    "    model_grid.fit(X_train, y_train)\n",
    "    return model_grid.best_estimator_, model_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Models\n",
    "> _Self-note: There are many other metrics we can play around w/ if we run out of things. I commented out decision trees that can split on the best random attribute_. We can place a range on many of the parameters, then graph them and see which one gives the best results like this: https://www.kaggle.com/tosinabase/mushroom-classification-tree-methods-comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### DT Models w/ 'veil-type' #######################\n",
    "## Creating a decision tree w/ gini index metric (gini impurity)\n",
    "# splitter = best (on the best feature)\n",
    "start = time.time()\n",
    "gini_decision_tree_best = DecisionTreeClassifier()\n",
    "gini_decision_tree_best.fit(X_train, y_train)\n",
    "train_gini_DT = time.time() - start\n",
    "print(f\"Time elpased to build/train gini DT: {train_gini_DT}\\n\")\n",
    "\n",
    "## Creating a decision tree w/ entropy metric (info gain) (Learned in class)\n",
    "# splitter = best (on the best feature)\n",
    "start = time.time()\n",
    "entropy_decision_tree_best = DecisionTreeClassifier(criterion = \"entropy\") \n",
    "entropy_decision_tree_best.fit(X_train, y_train)\n",
    "train_entropy_DT = time.time() - start\n",
    "print(f\"Time elpased to build/train entropy DT: {train_entropy_DT}\\n\")\n",
    "\n",
    "####################### DT Models w/o 'veil-type' #######################\n",
    "## Creating a decision tree w/ gini index metric (gini impurity) & w/0 'veil-type' feature\n",
    "# splitter = best (on the best feature)\n",
    "start = time.time()\n",
    "r_gini_decision_tree_best = DecisionTreeClassifier()\n",
    "r_gini_decision_tree_best.fit(r_X_train, r_y_train)\n",
    "r_train_gini_DT = time.time() - start\n",
    "print(f\"Time elpased to build/train gini DT w/o 'veil-type' feature: {r_train_gini_DT}\\n\")\n",
    "\n",
    "## Creating a decision tree w/ entropy metric (info gain) (Learned in class) & w/0 'veil-type' feature\n",
    "# splitter = best (on the best feature)\n",
    "start = time.time()\n",
    "r_entropy_decision_tree_best = DecisionTreeClassifier(criterion = \"entropy\") \n",
    "r_entropy_decision_tree_best.fit(r_X_train, r_y_train)\n",
    "r_train_entropy_DT = time.time() - start\n",
    "print(f\"Time elpased to build/train entropy DT w/o 'veil-type' feature: {r_train_entropy_DT}\\n\")\n",
    "\n",
    "### This not needed anymore\n",
    "\"\"\"\n",
    "####################### DT Models w/ clean data #######################\n",
    "## Creating a decision tree w/ gini index metric (gini impurity) & w/ clean data\n",
    "# splitter = best (on the best feature)\n",
    "start = time.time()\n",
    "c_gini_decision_tree_best = DecisionTreeClassifier()\n",
    "c_gini_decision_tree_best.fit(c_X_train, c_y_train)\n",
    "c_train_gini_DT = time.time() - start\n",
    "print(f\"Time elpased to build/train gini DT w/ clean data: {c_train_gini_DT}\\n\")\n",
    "\n",
    "## Creating a decision tree w/ entropy metric (info gain) (Learned in class) & w/ clean data\n",
    "# splitter = best (on the best feature)\n",
    "start = time.time()\n",
    "c_entropy_decision_tree_best = DecisionTreeClassifier(criterion = \"entropy\") \n",
    "c_entropy_decision_tree_best.fit(c_X_train, c_y_train)\n",
    "c_train_entropy_DT = time.time() - start\n",
    "print(f\"Time elpased to build/train entropy DT w/ clean data: {c_train_entropy_DT}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Decision Tree Models on Datasets w/ and w/o 'Veil-type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################### Testing DT Models w/ 'veil-type' #######################\n",
    "## Testing gini index decision tree w/ train test split\n",
    "print(\"Performance for Decision Tree w/ gini index using train test split:\")\n",
    "DT_gini_train_split, DT_gini_validation_split, DT_gini_testing_training_time, DT_gini_testing_validation_time = evaluate_model_n_print_info(gini_decision_tree_best, X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "## Testing information gain decision tree w/ train test split\n",
    "print(\"Performance for Decision Tree w/ information gain using train test split:\")\n",
    "DT_entropy_train_split, DT_entropy_validation_split, DT_entropy_testing_training_time, DT_entropy_testing_validation_time = evaluate_model_n_print_info(entropy_decision_tree_best, X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "####################### K-fold cross-validation #######################\n",
    "## Create new DT for k-fold cross-validation\n",
    "DT_gini_k_fold = DecisionTreeClassifier()\n",
    "DT_entropy_k_fold = DecisionTreeClassifier(criterion = \"entropy\") \n",
    "\n",
    "## K-fold cross-validation (k = 10) on gini index decision tree\n",
    "print(\"Performance for Decision Tree w/ gini index using K-fold cross-validation w/ K=10:\")\n",
    "DT_gini_k_fold_result = k_fold_cross_validation(DT_gini_k_fold, X_train, y_train)\n",
    "\n",
    "## K-fold cross-validation (k = 10) on info gain decision tree\n",
    "print(\"Performance for Decision Tree w/ information gain using K-fold cross-validation w/ K=10:\")\n",
    "DT_entropy_k_fold_result = k_fold_cross_validation(DT_entropy_k_fold, X_train, y_train)\n",
    "\n",
    "####################### Grid-Search that uses K-fold cross-validation to fine tune parameters #######################\n",
    "# This will take a while..\n",
    "# Parameters want GridSearch to find the best hyperparmeter for the model\n",
    "print(\"Performance or Grid-Search Decision Tree:\")\n",
    "DT_parameters = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth': list(range(1, 11)) + [None]\n",
    "}\n",
    "DT_best_grid_search, DT_grid_search_info = grid_serach_helper(DecisionTreeClassifier(), DT_parameters, X_train, y_train)\n",
    "evaluate_model_n_print_info(DT_best_grid_search, X_train, y_train, X_validation, y_validation)\n",
    "print(\"Compare parameters:\")\n",
    "print(f\"Max depth of DT w/ gini: {gini_decision_tree_best.tree_.max_depth}\")\n",
    "print(f\"Max depth of DT w/ entropy: {entropy_decision_tree_best.tree_.max_depth}\")\n",
    "print(f'Grid-Search DT Best paramters: {DT_grid_search_info.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Testing DT Models w/o 'veil-type' #######################\n",
    "## Testing gini index decision tree w/ train test split\n",
    "print(\"Performance for Decision Tree w/ gini index & w/o 'veil-type' using train test split:\")\n",
    "r_DT_gini_train_split, r_DT_gini_validation_split, r_DT_gini_testing_training_time, r_DT_gini_testing_validation_time = evaluate_model_n_print_info(r_gini_decision_tree_best, r_X_train, r_y_train, r_X_validation, r_y_validation)\n",
    "\n",
    "## Testing information gain decision tree w/ train test split\n",
    "print(\"Performance for Decision Tree w/ information gain & w/o 'veil-type' using train test split:\")\n",
    "r_DT_entropy_train_split, r_DT_entropy_validation_split, r_DT_entropy_testing_training_time, r_DT_entropy_testing_validation_time = evaluate_model_n_print_info(r_entropy_decision_tree_best, r_X_train, r_y_train, r_X_validation, r_y_validation)\n",
    "\n",
    "########## K-fold cross-validation to fine tune parameters ##########\n",
    "## Create new DT for k-fold cross-validation\n",
    "r_DT_gini_k_fold = DecisionTreeClassifier()\n",
    "r_DT_entropy_k_fold = DecisionTreeClassifier(criterion = \"entropy\") \n",
    "\n",
    "## K-fold cross-validation (k = 10) on gini index decision tree\n",
    "print(\"Performance for Decision Tree w/ gini index & w/o 'veil-type' using K-fold cross-validation w/ K=10:\")\n",
    "r_DT_gini_k_fold_result = k_fold_cross_validation(r_DT_gini_k_fold, r_X_train, r_y_train)\n",
    "\n",
    "## K-fold cross-validation (k = 10) on info gain decision tree\n",
    "print(\"Performance for Decision Tree w/ information gain & w/o 'veil-type' using K-fold cross-validation w/ K=10:\")\n",
    "r_DT_entropy_k_fold_result = k_fold_cross_validation(r_DT_entropy_k_fold, r_X_train, r_y_train)\n",
    "\n",
    "####################### Grid-Search that uses K-fold cross-validation to fine tune parameters #######################\n",
    "## This would just be a repeat of the results from above, since we realized that removing the 'veil-type' produces the same model\n",
    "## So we will continute to develop w/o 'veil-type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This can be removed kept this for testing a way to clean the data ### \n",
    "\"\"\"\n",
    "####################### Testing DT Models w/ clean data #######################\n",
    "## Testing gini index decision tree w/ train test split\n",
    "print(\"Performance for Decision Tree w/ gini index & w/ clean data using train test split:\")\n",
    "c_DT_gini_train_split, c_DT_gini_validation_split, c_DT_gini_testing_training_time, c_DT_gini_testing_validation_time = evaluate_model_n_print_info(c_gini_decision_tree_best, c_X_train, c_y_train, c_X_validation, c_y_validation)\n",
    "\n",
    "## Testing information gain decision tree w/ train test split\n",
    "print(\"Performance for Decision Tree w/ information gain & w/ clean data using train test split:\")\n",
    "c_DT_entropy_train_split, c_DT_entropy_validation_split, c_DT_entropy_testing_training_time, c_DT_entropy_testing_validation_time = evaluate_model_n_print_info(c_entropy_decision_tree_best, c_X_train, c_y_train, c_X_validation, c_y_validation)\n",
    "\n",
    "########## K-fold cross-validation to fine tune parameters ##########\n",
    "## Create new DT for k-fold cross-validation\n",
    "c_DT_gini_k_fold = DecisionTreeClassifier()\n",
    "c_DT_entropy_k_fold = DecisionTreeClassifier(criterion = \"entropy\") \n",
    "\n",
    "## K-fold cross-validation (k = 10) on gini index decision tree\n",
    "print(\"Performance for Decision Tree w/ gini index & w/ clean data using K-fold cross-validation w/ K=10:\")\n",
    "c_DT_gini_k_fold_result = k_fold_cross_validation(c_DT_gini_k_fold, c_X_train, c_y_train)\n",
    "\n",
    "## K-fold cross-validation (k = 10) on info gain decision tree\n",
    "print(\"Performance for Decision Tree w/ information gain & w/ clean data using K-fold cross-validation w/ K=10:\")\n",
    "c_DT_entropy_k_fold_result = k_fold_cross_validation(c_DT_entropy_k_fold, c_X_train, c_y_train)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Decision Tree Models\n",
    "#### Note: The graph that gets printed has misleading splitting attributes (it shows its splitting on numbers but all of the attribute values are strings; this is due to label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Display Decision Tree #############\n",
    "## Display decision tree w/ gini metric (gini impurity)\n",
    "# Generate a .dot file\n",
    "export_graphviz(gini_decision_tree_best, out_file = \"giniDecisionTree.dot\", \n",
    "                feature_names = list(hot_encoder.get_feature_names(list(dataset.columns[1:]))),\n",
    "                class_names = ['e', 'p'], filled = True, rounded = True)\n",
    "# Generate a .png of the .dot file\n",
    "giniDecisionTreePNG = Source.from_file(\"giniDecisionTree.dot\", format='png')\n",
    "giniDecisionTreePNG.view()\n",
    "\n",
    "## Display decision tree w/ entropy metric (info gain)\n",
    "export_graphviz(entropy_decision_tree_best, out_file = \"entropyDecisionTree.dot\", \n",
    "                feature_names = list(hot_encoder.get_feature_names(list(dataset.columns[1:]))), \n",
    "                class_names = ['e', 'p'], filled = True, rounded = True)\n",
    "# Generate a .png of the .dot file\n",
    "entropyDecisionTreePNG = Source.from_file(\"entropyDecisionTree.dot\", format='png')\n",
    "entropyDecisionTreePNG.view()\n",
    "\n",
    "######### Display Grid-Search Decision Tree #############\n",
    "export_graphviz(DT_best_grid_search, out_file = \"GridSearchDecisionTree.dot\", \n",
    "                feature_names = list(hot_encoder.get_feature_names(list(dataset.columns[1:]))), \n",
    "                class_names = ['e', 'p'], filled = True, rounded = True)\n",
    "# Generate a .png of the .dot file\n",
    "GridSearchDecisionTreePNG = Source.from_file(\"GridSearchDecisionTree.dot\", format='png')\n",
    "GridSearchDecisionTreePNG.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Decision Tree: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the numerical values of the Importance Scores\n",
    "print(\"Decision Trees Feature Importance Scores:\")\n",
    "encoded_columns = hot_encoder.get_feature_names(list(dataset.columns[1:]))\n",
    "print(\"Feature {0:25} Gini DT {0:25} Entropy DT \".format(\" \"))\n",
    "for i, v in enumerate(gini_decision_tree_best.feature_importances_):\n",
    "    print(\"{0:<25} {1:^25} {2:>25}\".\n",
    "          format(encoded_columns[i], v, entropy_decision_tree_best.feature_importances_[i], \" \"))\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(\"Grid Search Decision Tree Feature Importance Scores:\")\n",
    "encoded_columns = hot_encoder.get_feature_names(list(dataset.columns[1:]))\n",
    "print(\"Feature {0:18} Feature Importance Score \".format(\" \"))\n",
    "for i, v in enumerate(DT_best_grid_search.feature_importances_):\n",
    "    print(\"{0:<25} {1:^25}\".\n",
    "          format(encoded_columns[i], v, \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the numerical values of the Importance Scores in a bar graph\n",
    "DT_important_features_df = pd.DataFrame(\n",
    "    data = {\"Gini DT\" : gini_decision_tree_best.feature_importances_, \n",
    "            \"Entropy DT\" : entropy_decision_tree_best.feature_importances_}, index = encoded_columns)\n",
    "\n",
    "DT_grid_search_important_features_df = pd.DataFrame(\n",
    "    data = {\"Grid Search DT\" : DT_best_grid_search.feature_importances_}, index = encoded_columns)\n",
    "\n",
    "DT_important_features_df.sort_values(by=[\"Gini DT\"], ascending = False, inplace = True)\n",
    "DT_grid_search_important_features_df.sort_values(by=[\"Grid Search DT\"], ascending = False, inplace = True)\n",
    "\n",
    "# Display the number of features each split on\n",
    "print(f\"Number of features split on for Gini DT: {DT_important_features_df[(DT_important_features_df['Gini DT'] != 0)].shape[0]}\")\n",
    "print(f\"Number of features split on for Entropy DT: {DT_important_features_df[(DT_important_features_df['Entropy DT'] != 0)].shape[0]}\")\n",
    "print(f\"Number of features split on for Grid Search DT: {DT_grid_search_important_features_df[(DT_grid_search_important_features_df['Grid Search DT'] != 0)].shape[0]}\")\n",
    "\n",
    "DT_important_features_df.drop(DT_important_features_df[(DT_important_features_df['Gini DT'] == 0) \n",
    "                                                       & (DT_important_features_df['Entropy DT'] == 0)].index, inplace = True)\n",
    "DT_grid_search_important_features_df.drop(DT_grid_search_important_features_df[(DT_grid_search_important_features_df['Grid Search DT'] == 0)].index, inplace = True)      \n",
    "# display(DT_grid_search_important_features_df)\n",
    "# display(DT_important_features_df)\n",
    "bar_DT_important_feat = DT_important_features_df.plot.bar(rot=0, fontsize=15, figsize = (50, 10))\n",
    "bar_DT_grid_search_important_feat = DT_grid_search_important_features_df.plot.bar(rot=0, fontsize=15, figsize = (50, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a vector of the classes\n",
    "y_train_vect = y_train.reshape((5686, ))\n",
    "y_validation_vect = y_validation.reshape((2438, ))\n",
    "\n",
    "## Creating a random forest w/ gini index metric (gini impurity)\n",
    "n_trees_gini = 100 # default amount\n",
    "start = time.time()\n",
    "gini_random_forest = RandomForestClassifier(n_estimators = n_trees_gini)\n",
    "gini_random_forest.fit(X_train, y_train_vect)\n",
    "train_gini_RF = time.time() - start\n",
    "print(f\"Time elpased to build/train gini RF: {train_gini_RF}\\n\")\n",
    "\n",
    "## Creating a random forest w/ entropy metric (info gain) (Learned in class)\n",
    "n_trees_entropy = 100 # default amount\n",
    "start = time.time()\n",
    "entropy_random_forest = RandomForestClassifier(n_estimators = n_trees_entropy, criterion = \"entropy\") \n",
    "entropy_random_forest.fit(X_train, y_train_vect)\n",
    "train_entropy_RF = time.time() - start\n",
    "print(f\"Time elpased to build/train entropy RF: {train_entropy_RF}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Testing gini index random forest w/ train test split\n",
    "print(\"Performance for Random Forest w/ gini index using train test split:\")\n",
    "RT_gini_train_split, RT_gini_validation_split, RT_gini_testing_training_time, RT_gini_testing_validation_time = evaluate_model_n_print_info(gini_random_forest, X_train, y_train_vect, X_validation, y_validation_vect)\n",
    "\n",
    "## Testing information gain random forest w/ train test split\n",
    "print(\"Performance for Random Forest w/ information gain using train test split:\")\n",
    "RT_entropy_train_split, RT_entropy_validation_split, RT_entropy_testing_training_time, RT_entropy_testing_validation_time = evaluate_model_n_print_info(entropy_random_forest, X_train, y_train_vect, X_validation, y_validation_vect)\n",
    "\n",
    "\n",
    "########## K-fold cross-validation to fine tune parameters ##########\n",
    "## Create new DT for k-fold cross-validation\n",
    "RF_gini_k_fold = RandomForestClassifier()\n",
    "RF_entropy_k_fold = RandomForestClassifier(criterion = \"entropy\")\n",
    "\n",
    "## K-fold cross-validation (k = 10) on gini index random forest\n",
    "print(\"Performance for Random Forest w/ gini index using K-fold cross-validation w/ K=10:\")\n",
    "RF_gini_k_fold_result = k_fold_cross_validation(RF_gini_k_fold, X_train, y_train_vect)\n",
    "\n",
    "## K-fold cross-validation (k = 10) on info gain random forest\n",
    "print(\"Performance for Random Forest w/ information gain using K-fold cross-validation w/ K=10:\")\n",
    "RF_gini_k_fold_result = k_fold_cross_validation(RF_entropy_k_fold, X_train, y_train_vect)\n",
    "\n",
    "####################### Grid-Search that uses K-fold cross-validation to fine tune parameters #######################\n",
    "## Note this takes a while to run, but I do graph this later on so might have an error when displaying RF, but works\n",
    "\"\"\"\n",
    "RF_parameters = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(2, 11)) + [None],\n",
    "    'n_estimators': [15, 25, 50, 75, 100] # the default is 100\n",
    "}\n",
    "RF_best_search_grid, RF_search_grid_info = grid_serach_helper(RandomForestClassifier(), RF_parameters, X_train, y_train_vect)\n",
    "evaluate_model_n_print_info(RF_best_search_grid, X_train, y_train_vect, X_validation, y_validation_vect)\n",
    "print(f'Best paramters: {RF_search_grid_info.best_params_}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Random Forest Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### These will take a while to run & might be laggy\n",
    "## Display random forest w/ gini metric (gini impurity)\n",
    "# Since we are not able to display all tree, we will randomly select the first 5 of them to display\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=3000)\n",
    "for index in range(0, 5):\n",
    "    tree.plot_tree(gini_random_forest.estimators_[index], feature_names = list(hot_encoder.get_feature_names(list(dataset.columns[1:]))),\n",
    "                class_names = ['e', 'p'], filled = True, ax = axes[index])\n",
    "    axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "fig.savefig('giniRandomForestPNG.png')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "## Display random forest w/ entropy metric (gini impurity)\n",
    "# Since we are not able to display all tree, we will randomly select the first 5 of them to display\n",
    "fig2, axes2 = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=3000)\n",
    "for index in range(0, 5):\n",
    "    tree.plot_tree(entropy_random_forest.estimators_[index], feature_names = list(hot_encoder.get_feature_names(list(dataset.columns[1:]))),\n",
    "                class_names = ['e', 'p'], filled = True, ax = axes2[index])\n",
    "    axes2[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "fig2.savefig('entropyRandomForestPNG.png')\n",
    "fig2.show()\n",
    "\n",
    "\"\"\"\n",
    "## Display random forest w/ Grid Search\n",
    "## Note: You need to run the GridSearchCSV from above to run this code\n",
    "fig3, axes3 = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=3000)\n",
    "for index in range(0, 5):\n",
    "    tree.plot_tree(RF_best_search_grid.estimators_[index], feature_names = list(hot_encoder.get_feature_names(list(dataset.columns[1:]))),\n",
    "                class_names = ['e', 'p'], filled = True, ax = axes3[index])\n",
    "    axes3[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "fig2.savefig('gridSearchRandomForestPNG.png')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Possibly add this instead of .view()\n",
    "# # Convert to png using system command (requires Graphviz)\n",
    "# from subprocess import call\n",
    "# call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# # Display in jupyter notebook\n",
    "# from IPython.display import Image\n",
    "# Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Random Forest: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the numerical values of the Importance Scores\n",
    "print(\"Random Forest Feature Importance Scores:\")\n",
    "encoded_columns = hot_encoder.get_feature_names(list(dataset.columns[1:]))\n",
    "print(\"Feature {0:25} Gini RF {0:20} Entropy RF \".format(\" \"))\n",
    "for i, v in enumerate(gini_random_forest.feature_importances_):\n",
    "    print(\"{0:<25} {1:^25} {2:>25}\".\n",
    "          format(encoded_columns[i], v, entropy_random_forest.feature_importances_[i], \" \"))\n",
    "\n",
    "# Display the numerical values of the Importance Scores of Grid Search\n",
    "print(\"Random Forest Feature Importance Scores:\")\n",
    "encoded_columns = hot_encoder.get_feature_names(list(dataset.columns[1:]))\n",
    "print(\"Feature {0:25} Grid Search RF \".format(\" \"))\n",
    "for i, v in enumerate(RF_best_search_grid.feature_importances_):\n",
    "    print(\"{0:<25} {1:^25}\".\n",
    "          format(encoded_columns[i], v, \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Display the numerical values of the Importance Scores in a bar graph\n",
    "RF_important_features_df = pd.DataFrame(\n",
    "    data = {\"Gini RF\" : gini_random_forest.feature_importances_, \n",
    "            \"Entropy RF\" : entropy_random_forest.feature_importances_}, index = encoded_columns)\n",
    "RF_important_features_df.drop(RF_important_features_df[(RF_important_features_df['Gini RF'] == 0) \n",
    "                                                       & (RF_important_features_df['Entropy RF'] == 0)].index, inplace = True)\n",
    "\n",
    "RF_important_features_df.sort_values(by=[\"Gini RF\"], ascending = False, inplace = True)\n",
    "\n",
    "# display(RF_important_features_df)\n",
    "bar_RF_important_feat = RF_important_features_df.plot.bar(rot=0, fontsize=15, figsize = (500, 10))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "##### THIS WILL GIVE YOU AN ERROR IF YOU DO NOT FINISH RUNNING THE RF FROM ABOVE #####\n",
    "## Display the numerical values of the Importance Scores  of Grid Search in a bar graph\n",
    "Grid_Search_RF_important_features_df = pd.DataFrame(\n",
    "    data = {\"Grid Search RF\" : RF_best_search_grid.feature_importances_}, index = encoded_columns)\n",
    "Grid_Search_RF_important_features_df.drop(Grid_Search_RF_important_features_df[(Grid_Search_RF_important_features_df['Grid Search RF'] == 0)].index, inplace = True)\n",
    "\n",
    "Grid_Search_RF_important_features_df.sort_values(by=[\"Grid Search RF\"], ascending = False, inplace = True)\n",
    "\n",
    "# display(RF_important_features_df)\n",
    "bar_grid_search_RF_important_feat = Grid_Search_RF_important_features_df.plot.bar(rot=0, fontsize=15, figsize = (500, 10))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default instance of a logistic regression\n",
    "start = time.time()\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train.ravel())\n",
    "train_log_reg = time.time() - start\n",
    "print(f\"Time elpased to build/train gini DT: {train_log_reg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing gini index decision tree w/ train test split\n",
    "print(\"Performance for Logistic Regression Model using train test split:\")\n",
    "log_reg_train_split, log_reg_validation_split, log_reg_testing_training_time, log_reg_testing_validation_time = evaluate_model_n_print_info(log_reg, X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "########## K-fold cross-validation to fine tune parameters ##########\n",
    "## Create new DT for k-fold cross-validation\n",
    "log_reg_k_fold = LogisticRegression()\n",
    "\n",
    "## K-fold cross-validation (k = 10) on gini index decision tree\n",
    "print(\"Performance for Logistic Regression Model using K-fold cross-validation w/ K=10:\")\n",
    "log_reg_k_fold_result = k_fold_cross_validation(log_reg_k_fold, X_train, y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Logistic Regression: Feature Importance\n",
    "> This is one of the challenges w/ using a model such as logistic regression, other than DT, it is not able to easily able to visualize and find the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coefs = np.abs(log_reg.coef_[0])\n",
    "# Display the numerical values of the Importance Scores\n",
    "# Posiive coefficients indicate it predicts class 1 and negative score indicate a feature that predicts class 0\n",
    "print(\"Logistic Regression Feature Importance Scores:\")\n",
    "encoded_columns = hot_encoder.get_feature_names(list(dataset.columns[1:]))\n",
    "print(\"Feature {0:20} Coefficient {0:20} Abs(coefficient)\".format(\" \"))\n",
    "for i, v in enumerate(log_reg.coef_[0]):\n",
    "    print(\"{0:<25} {1:^20} {2:>33}\".\n",
    "          format(encoded_columns[i], v, abs(v), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Display the numerical values of the Coefficents of log regression in a bar graph\n",
    "log_reg_coefficient_df = pd.DataFrame(data = {\"Coefficient\" : log_reg.coef_[0]}, index = encoded_columns)\n",
    "log_reg_abs_coefficient_df = pd.DataFrame(data = {\"Abs(Coefficient)\" : abs(log_reg.coef_[0])}, index = encoded_columns)\n",
    "log_reg_coefficient_df.sort_values(by=[\"Coefficient\"], ascending = False, inplace = True)\n",
    "log_reg_abs_coefficient_df.sort_values(by=[\"Abs(Coefficient)\"], ascending = False, inplace = True)\n",
    "\n",
    "bar_log_reg_coefficient = log_reg_coefficient_df.plot.bar(title = \"Coefficient val vs. Features Type\", rot=0, fontsize=15, figsize = (500, 10))\n",
    "bar_log_reg_abs_coefficient = log_reg_abs_coefficient_df.plot.bar(title = \"Abs(Coefficient) val vs. Feature Type\", rot=0, fontsize=15, figsize = (500, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_summary(dt1, dt2, rf1, rf2, log, label, title, zoom=0):\n",
    "    summary_data = { \n",
    "        \"Models\" : [\"Decision Tree gini\", \"Decision Tree entropy\", \"Random Forest gini\", \"Random Forest entropy\", \n",
    "                    \"Logistic Regression\"],\n",
    "        label : [dt1, dt2, rf1, rf2, log]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(data = summary_data)\n",
    "    plt.figure(figsize = (10,5))\n",
    "    sns.barplot(x = \"Models\", y = label, data = summary_df, palette='Set2')\n",
    "    plt.title(title, fontsize = 25)\n",
    "    plt.xlabel('Models', fontsize = 15)\n",
    "    plt.ylabel(label,fontsize = 15)\n",
    "    plt.xticks(rotation=90, fontsize = 15)\n",
    "    # zoom set to 1 for first 3 accuracy models\n",
    "    if zoom:\n",
    "        plt.ylim(0.95,1.005)\n",
    "    plt.show()\n",
    "    display(summary_df)\n",
    "    \n",
    "def cal_avg_kfold_accuracy(result):\n",
    "    return round(sum(result['test_score']) / len(result['test_score']), 4)\n",
    "\n",
    "def cal_avg_kfold_time(result):\n",
    "    return round(sum(result['fit_time']) / len(result['fit_time']), 4)\n",
    "\n",
    "print(\"Training Set Accuracy by Model\")\n",
    "display_summary(DT_gini_train_split, DT_entropy_train_split, RT_gini_train_split, RT_entropy_train_split, \n",
    "                log_reg_train_split, \"Accuracy (%)\", \"Training Set Accuracy by Model\", 1)\n",
    "\n",
    "print(\"Validation Set Accuracy by Model\")\n",
    "display_summary(DT_gini_validation_split, DT_entropy_validation_split, RT_gini_validation_split, \n",
    "                RT_entropy_validation_split, log_reg_validation_split, \"Accuracy (%)\", \"Validation Set Accuracy by Model\", 1)\n",
    "\n",
    "print(\"Average K-Fold Accuracy (k = 10) by Model\")\n",
    "display_summary(cal_avg_kfold_accuracy(DT_gini_k_fold_result), \n",
    "                cal_avg_kfold_accuracy(DT_entropy_k_fold_result), \n",
    "                cal_avg_kfold_accuracy(RF_gini_k_fold_result),\n",
    "                cal_avg_kfold_accuracy(RF_gini_k_fold_result), \n",
    "                cal_avg_kfold_accuracy(log_reg_k_fold_result), \"Accuracy (%)\", \"Average K-Fold Accuracy (k = 10) by Model\", 1)\n",
    "\n",
    "print(\"Time to Train Model\")\n",
    "display_summary(DT_gini_testing_training_time, DT_entropy_testing_training_time, RT_gini_testing_training_time, \n",
    "                RT_entropy_testing_training_time, log_reg_testing_training_time, \"Time (sec)\", \"Time to Train Model\")\n",
    "\n",
    "print(\"Time to Run Training Set on Model\")\n",
    "display_summary(train_gini_DT, train_entropy_DT, train_gini_RF, train_entropy_RF, train_log_reg, \"Time (sec)\", \n",
    "                \"Time to Run Training Set on Model\")\n",
    "\n",
    "print(\"Time to Run Validate Set on Model\")\n",
    "display_summary(DT_gini_testing_validation_time, DT_entropy_testing_validation_time, RT_gini_testing_validation_time, \n",
    "                RT_entropy_testing_validation_time, log_reg_testing_validation_time, \"Time (sec)\", \n",
    "                \"Time to Run Validate Set on Model\")\n",
    "\n",
    "print(\"Average Time to Run K-Fold\")\n",
    "display_summary(cal_avg_kfold_time(DT_gini_k_fold_result), \n",
    "                cal_avg_kfold_time(DT_entropy_k_fold_result), \n",
    "                cal_avg_kfold_time(RF_gini_k_fold_result),\n",
    "                cal_avg_kfold_time(RF_gini_k_fold_result),\n",
    "                cal_avg_kfold_time(log_reg_k_fold_result),\"Time (sec)\", \"Average Time to Run K-Fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
